{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a07442c6-9dd1-4d1b-9cb2-aadfd6616fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"\n",
    "    Di sebuah desa yang terkenal dengan hasil pertaniannya,\n",
    "    para petani menanam berbagai jenis buah seperti pisang, jeruk, dan apel.\n",
    "    Setiap musim panen tiba, apel menjadi buah yang paling banyak dicari karena rasanya\n",
    "    yang segar dan kandungan gizinya yang tinggi. Tidak hanya dijual dalam bentuk buah segar,\n",
    "    apel juga diolah menjadi jus, selai, dan kue oleh para pelaku UMKM setempat. Menariknya,\n",
    "    beberapa petani mulai bereksperimen dengan varietas apel baru yang lebih tahan terhadap perubahan cuaca.\n",
    "    Walaupun harga apel kadang naik turun di pasar, permintaan terhadap apel tetap stabil\n",
    "    karena masyarakat sudah terbiasa mengonsumsi Apeljack sebagai bagian dari gaya hidup sehat.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24e48dd9-9c0a-425a-9c31-52f06aa8e091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# count word \"Apel\" on paragraph using count \n",
    "# but when text include \"Apeljack\" it will still count\n",
    "\n",
    "wordApel = paragraph.lower().count(\"apel\")\n",
    "print(wordApel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e86db0e-69ab-4429-be22-2157818e9ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# count word \"Apel\" on pargraph using counter and regex\n",
    "import re\n",
    "\n",
    "wordApel = sum(1 for match in re.finditer(r\"\\bapel\\b\", paragraph.lower()))\n",
    "\n",
    "print(wordApel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b387076-8fe2-499a-a0c5-0fb6570dfa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# count word \"Apel\" on paragraph using counter and collections\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "wordApel = re.findall(r'\\b\\w+\\b', paragraph.lower())\n",
    "counter = Counter(wordApel)\n",
    "\n",
    "print(counter[\"apel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e00bdb4",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8de2e4-f5cb-4ac7-9172-a68efa2b9386",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "is a method for evaluating how important a word is in a documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411cc05b-7a00-44ed-b8cc-3ef31f721464",
   "metadata": {},
   "outputs": [],
   "source": [
    "### install scikit-learn and pandas\n",
    "!pip install scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5487b49f-fba4-4307-a01e-ae159879e655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               apel  berwarna   dimakan       itu     merah     sudah  \\\n",
      "Dokumen A  0.449436  0.631667  0.000000  0.000000  0.631667  0.000000   \n",
      "Dokumen B  0.335176  0.000000  0.471078  0.471078  0.000000  0.471078   \n",
      "\n",
      "               ulat  \n",
      "Dokumen A  0.000000  \n",
      "Dokumen B  0.471078  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. Setup data (korspus)\n",
    "# Dokumen A (Index 0) : \"Apel berwarna merah\"\n",
    "# Dokumen B (Index 1) : \"Apel itu sudah dimakan ulat\"\n",
    "\n",
    "corpus = [\n",
    "    \"Apel berwarna merah\",\n",
    "    \"Apel itu sudah dimakan ulat\"\n",
    "]\n",
    "\n",
    "# 2. Initialize Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# down here will start calculate using TF-IDF\n",
    "# 3. Calculate TF-IDF (Fit & Transform)\n",
    "# Machine will calculate (Fit) the value in docs\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 4. Show result\n",
    "# get name \"word\" (feature) from machine\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# create table\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names, index=[\"Dokumen A\", \"Dokumen B\"])\n",
    "\n",
    "\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e5fd25-0715-4af6-822a-58e6721d5754",
   "metadata": {},
   "source": [
    "Kata Apel itu mendapatakan nilai sekitar 0.4 dan 0.3 tidak terlalu tinggi nilainya (jika dilihat per dokumen) itu karna kata tersebut muncul di kedua dokumen, lalu ada kata bewarna merah di dok A sebagai \"ciri khas\" dokumen tersebut, sedangkan di dokumen B ada kata dimakan, itu, sudah sebagai ciri khas dokumen B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe0a70-ef92-4c0a-a1eb-cf65a85f9dae",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e576e77-29e9-4f4f-a874-992fb7b45f2b",
   "metadata": {},
   "source": [
    "Sebuah metode untuk mengukur seberapa mirip dua benda (dalam hal ini dokument/teks) dengan menghitung kosinus sudut di antara kedua vektor dokumen tersebut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "938fda44-f618-427c-bfaa-c864b8172c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7999999999999998\n"
     ]
    }
   ],
   "source": [
    "## implement\n",
    "# first using numpy\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# 1. vektor\n",
    "# Dokumen A: 1 Apel, 2 Manis\n",
    "\n",
    "vec_a = np.array([1,2])\n",
    "\n",
    "# Dokumen B: 2 Apel, 1 Manis\n",
    "\n",
    "vec_b = np.array([2,1])\n",
    "\n",
    "# 2. Calculate Dot Product\n",
    "# Rumus: (1*2) + (2*1) = 4\n",
    "\n",
    "dot_product = np.dot(vec_a, vec_b)\n",
    "\n",
    "# 3. Calculate (Magnitude / Length Vector)\n",
    "# Rumus: Akar ( ||A|| * ||B||)\n",
    "\n",
    "norm_a = norm(vec_a)\n",
    "norm_b = norm(vec_b)\n",
    "\n",
    "# 4. Hitung Cosine Similarity\n",
    "# Rumus: dot_product / (norm_a * norm_b)\n",
    "\n",
    "result = dot_product / (norm_a * norm_b)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa95d1-43db-4008-be0b-1fae0930cf8e",
   "metadata": {},
   "source": [
    "Hasilnya 0.799... itu berarti dokumen A dan dokumen B memiliki kemiripan sekitar 79 - 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1bec30d-ff9f-49d9-aff2-0674fd113611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil Cosine similarity : 0.7999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Using scikit-learn\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data \n",
    "vec_a = np.array([[1,2]])\n",
    "vec_b = np.array([[2,1]])\n",
    "\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity = cosine_similarity(vec_a, vec_b)\n",
    "\n",
    "print(\"Hasil Cosine similarity :\", similarity[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1e2fa-cf32-431d-ae9d-a86cd28953dc",
   "metadata": {},
   "source": [
    "Hasilnya samaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5bf5581-1da2-4d11-aa83-be8eb6594a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp310-cp310-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp310-cp310-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl.metadata (91 kB)\n",
      "Collecting numpy>=1.22.0 (from scikit-learn)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (63 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading scikit_learn-1.7.2-cp310-cp310-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.3-cp310-cp310-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (14.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (35.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.5/35.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, numpy, joblib, scipy, pandas, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [scikit-learn][0m [scikit-learn]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.3 numpy-2.2.6 pandas-2.3.3 pytz-2025.2 scikit-learn-1.7.2 scipy-1.15.3 threadpoolctl-3.6.0 tzdata-2025.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e60c43f6-f08a-4907-b9d0-3d2e46a07c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting Sastrawi\n",
      "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/nlp/lib/python3.10/site-packages (from nltk) (1.5.3)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2026.1.15-cp310-cp310-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
      "Downloading regex-2026.1.15-cp310-cp310-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: Sastrawi, tqdm, regex, click, nltk\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [nltk][32m4/5\u001b[0m [nltk]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Sastrawi-1.0.1 click-8.3.1 nltk-3.9.2 regex-2026.1.15 tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b52ff494-7289-4bcd-9b43-b730457c9b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: B\n",
      "Similarity: 0.17662402681611103\n",
      "Result: Pakta Bentukan Arab Saudi Rombak Aliansi Keamanan Timur Tengah?\n",
      "\n",
      "Query: B\n",
      "Similarity: 0.19568723861021348\n",
      "Result: Reza Arap Penuhi Panggilan Polisi soal Lula Lahfah, Ditanya 30 Pertanyaan\n",
      "\n",
      "Query: B\n",
      "Similarity: 0.19568723861021348\n",
      "Result: Reza Arap Penuhi Panggilan Polisi soal Lula Lahfah, Ditanya 30 Pertanyaan\n",
      "\n",
      "[\"Eks Anggota DPRD Bekasi F-PDIP Dipanggil KPK Terkait Kasus Ade Kuswara\", \"KPK Panggil Direktur Pemeriksaan-Penagihan DJP Terkait Kasus Suap Pajak Jakut\", \"Permukiman Liar Dibongkar, TPU Kebon Nanas Siap Tampung Ribuan Makam Baru\", \"Video: Sah! Thomas Djiwandono Jadi Deputi Gubernur BI\", \"Indeks Perkembangan Harga di Aceh, Sumbar & Sumut Turun Signifikan\", \"Eks Anggota DPRD Bekasi F-PDIP Dipanggil KPK Terkait Kasus Ade Kuswara\", \"KPK Panggil Direktur Pemeriksaan-Penagihan DJP Terkait Kasus Suap Pajak Jakut\", \"Lansia di Lebak Tenggelam Saat Coba Seberangi Sungai Pakai Batang Pisang\", \"Komisi I DPR Gelar Rapat Tertutup Bareng Menlu Sugiono\", \"Video Gedung Putih Tuding Demokrat Jadi Dalang Kerusuhan di Minneapolis\", \"Rocky Gerung Diperiksa di Kasus Ijazah Jokowi, Jelaskan Hal Ini ke Polisi\", \"Konsolidasi Kekuasaan To Lam, Taruhan Politik Vietnam\", \"BP MPR Gelar Rapim, Tetapkan 5 Topik Kajian 2026 & Program Kegiatan Baru\", \"Sekjen NATO: Eropa Tak Siap Bertahan Tanpa AS\", \"HNW Ingatkan Partisipasi RI dalam Dewan Perdamaian Harus Taat Konstitusi\", \"Pesawat Smart Air Jatuh di Pantai Nabire, Semua Penumpang Selamat\", \"Jadi Deputi Gubernur BI Terpilih, Tommy Djiwandono Tegaskan Jaga Independensi\", \"Hujan Salju Bikin 7.000 Orang Terjebak di Bandara Jepang\", \"Waka MPR Dorong Upaya Wujudkan Keluarga Jadi Ruang Aman bagi Perempuan\", \"Budi Djiwandono Respons Seloroh Utut soal Akan Masuk Eksekutif\", \"Polantas di Riau Masuk SD, Edukasi Green Policing dan Aman Berlalu Lintas\", \"Habiburokhman soal Adies Jadi Calon Hakim MK: Dia Nggak Sakiti Siapa Pun\", \"Video: Detik-detik Wali Kota Filipina Lolos dari Serangan RPG\", \"Wali Kota Metro Siap Sediakan Lahan Sekolah Rakyat Permanen\", \"Ahok di Sidang Kasus Minyak: Golf Tempat Negosiasi Paling Murah\", \"Video Rocky Gerung Sebut Ijazah Jokowi Asli: Tapi Orangnya yang Palsu\", \"Pramono Minta RS Jakarta Melayani Pakai Hati: BPJS dan Non-BPJS Sama\", \"I Wayan Sudirta Gantikan TB Hasanuddin Jadi Wakil Ketua MKD DPR\", \"Pantai Utara Tegal Dipenuhi Kayu Gelondongan Usai Banjir Bandang\", \"Sejalan Arahan Presiden, Pekanbaru Terapkan Manajemen Talenta ASN\", \"187 Ton Sampah Usai Banjir Diangkut dari Pinggir Kali di Jakbar\", \"Video: Penembakan Maut di Lapangan Sepak Bola Meksiko, 11 Orang Tewas\", \"BNN Wanti-wanti soal N2O 'Gas Tertawa', Bisa Menyebabkan Kematian\", \"Pemulihan Pascabencana Aceh, 70 Relawan BRI Peduli Bersih-bersih Sekolah\", \"Pakta Bentukan Arab Saudi Rombak Aliansi Keamanan Timur Tengah?\", \"Terjadi Lagi, Pelat Besi JPO di Daan Mogot Jakbar Hilang Dicuri\", \"Paripurna Tetapkan Polri di Bawah Presiden Jadi Keputusan Mengikat DPR-Pemerintah\", \"Pemkot Depok Terapkan WFH bagi ASN Setiap Kamis, Pelayanan Publik Tetap Normal\", \"Ibu di Kalbar Cabuli Anak Angkat Laki-laki, Rekam Aksi untuk Dipamerkan\", \"Pelindo Dukung Pengembangan Kawasan Pesisir-Pelabuhan Laut di Jakarta\", \"Video Rocky Gerung Jadi Saksi Roy Suryo di Kasus Ijazah Jokowi\", \"30 Orang Tewas Akibat Badai Musim Dingin di AS\", \"Potret Ahok Bersaksi di Sidang Kasus Tata Niaga Minyak\", \"Video Paripurna DPR Sahkan Adies Kadir Calon Hakim MK Usulan DPR\", \"Komisi III DPR Ungkap Alasan Adies Calon Hakim MK Gantikan Inosentius\", \"Feminisme Pesantren: Membangun Tradisi Adil dari Dalam\", \"Menanti Huntara, Penyintas Banjir Maninjau Bertahan di Bekas Kandang Sapi\", \"Reza Arap Penuhi Panggilan Polisi soal Lula Lahfah, Ditanya 30 Pertanyaan\", \"Profil Sari Yuliati, Bendum Golkar Kini Jabat Wakil Ketua DPR RI\", \"Kembangkan Penegakan Hukum Digital, Kakorlantas Terus Sempurnakan ETLE\", \"6 WNI Dihukum Bui-Cambuk Gegara Masuk Singapura secara Ilegal\", \"Video Permintaan Maaf Aparat yang Curigai Es Kue dari Bahan Spons\", \"prabowo subianto\", \"banjir\", \"lula lahfah\", \"Reza Arap Penuhi Panggilan Polisi soal Lula Lahfah, Ditanya 30 Pertanyaan\", \"Ahok di Sidang Kasus Minyak: Golf Tempat Negosiasi Paling Murah\", \"Rocky Gerung Diperiksa di Kasus Ijazah Jokowi, Jelaskan Hal Ini ke Polisi\", \"Pesawat Smart Air Jatuh di Pantai Nabire, Semua Penumpang Selamat\", \"Sari Yuliati Golkar Gantikan Adies Kadir Jadi Wakil Ketua DPR\", \"Kepala BNN hingga Mendes Launching Nasional Indonesia Bersinar, Dukung Asta Cita\", \"BNN Perkuat Sinergi dengan CNB Singapura untuk Lacak TPPU Bandar Narkoba\", \"BNN-Polda Sumut Gerebek Kampung Narkoba di Deli Serdang, Tangkap 19 Orang\"]\n"
     ]
    }
   ],
   "source": [
    "## Exercise\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "\n",
    "list_data = []\n",
    "\n",
    "with open(\"data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    list_data = json.load(f)\n",
    "\n",
    "# implementasi cosine similarity\n",
    "\n",
    "# implement stemming\n",
    "# using nltk and Sastrawi\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# Using stop word better query\n",
    "factory_sw = StopWordRemoverFactory()\n",
    "stopwords = factory_sw.get_stop_words()\n",
    "\n",
    "\n",
    "# Init dlu yang mau di stopwords : gabungkan dengan yg default\n",
    "custom_stopwords = stopwords + [\"kasus\", \"diduga\", \"terkait\", \"anggota\"]\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    words = text.split()\n",
    "\n",
    "    filterred_words = [w for w in words if w not in custom_stopwords]\n",
    "    text = \" \".join(filterred_words)\n",
    "    text = stemmer.stem(text)\n",
    "    return text\n",
    "\n",
    "query = \"Berita kasus reza arab\"\n",
    "query_stemmed = preprocess(query)\n",
    "\n",
    "\n",
    "# include to vectorizer\n",
    "\n",
    "# masih ada issue misalnya ketik reza arab tpi data \"arab\" juga muncul\n",
    "# ini tidak relevan dengan querynya\n",
    "# solusinya? pakai n-grams\n",
    "# ngram_range=(3, 5): Baca potongan 3 huruf sampai 5 huruf\n",
    "# syntaxnya mirip slice di js :v\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(3, 5))\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "clean_data = []\n",
    "for doc in list_data:\n",
    "    clean_data.append(preprocess(doc))\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(clean_data)\n",
    "\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "\n",
    "# Create vector\n",
    "vector_query = vectorizer.transform([query])\n",
    "\n",
    "# calculate cosine similarity\n",
    "similarity = cosine_similarity(vector_query, tfidf_matrix)\n",
    "\n",
    "results = similarity[0]\n",
    "\n",
    "for i, x in enumerate(results):\n",
    "    if x >= 0.1:\n",
    "        print(\"Query:\", query[0])\n",
    "        print(\"Similarity:\", x)\n",
    "        print(\"Result:\", list_data[i])\n",
    "        print()\n",
    "\n",
    "print(json.dumps(list_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae146cc-dd73-4e57-ac1d-059b8dc7aaa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
