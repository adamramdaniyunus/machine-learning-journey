{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a07442c6-9dd1-4d1b-9cb2-aadfd6616fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"\n",
    "    Di sebuah desa yang terkenal dengan hasil pertaniannya,\n",
    "    para petani menanam berbagai jenis buah seperti pisang, jeruk, dan apel.\n",
    "    Setiap musim panen tiba, apel menjadi buah yang paling banyak dicari karena rasanya\n",
    "    yang segar dan kandungan gizinya yang tinggi. Tidak hanya dijual dalam bentuk buah segar,\n",
    "    apel juga diolah menjadi jus, selai, dan kue oleh para pelaku UMKM setempat. Menariknya,\n",
    "    beberapa petani mulai bereksperimen dengan varietas apel baru yang lebih tahan terhadap perubahan cuaca.\n",
    "    Walaupun harga apel kadang naik turun di pasar, permintaan terhadap apel tetap stabil\n",
    "    karena masyarakat sudah terbiasa mengonsumsi Apeljack sebagai bagian dari gaya hidup sehat.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24e48dd9-9c0a-425a-9c31-52f06aa8e091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# count word \"Apel\" on paragraph using count \n",
    "# but when text include \"Apeljack\" it will still count\n",
    "\n",
    "wordApel = paragraph.lower().count(\"apel\")\n",
    "print(wordApel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e86db0e-69ab-4429-be22-2157818e9ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# count word \"Apel\" on pargraph using counter and regex\n",
    "import re\n",
    "\n",
    "wordApel = sum(1 for match in re.finditer(r\"\\bapel\\b\", paragraph.lower()))\n",
    "\n",
    "print(wordApel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b387076-8fe2-499a-a0c5-0fb6570dfa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# count word \"Apel\" on paragraph using counter and collections\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "wordApel = re.findall(r'\\b\\w+\\b', paragraph.lower())\n",
    "counter = Counter(wordApel)\n",
    "\n",
    "print(counter[\"apel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e00bdb4",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8de2e4-f5cb-4ac7-9172-a68efa2b9386",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "is a method for evaluating how important a word is in a documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "411cc05b-7a00-44ed-b8cc-3ef31f721464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp310-cp310-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp310-cp310-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl.metadata (91 kB)\n",
      "Collecting numpy>=1.22.0 (from scikit-learn)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (63 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading scikit_learn-1.7.2-cp310-cp310-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m658.5 kB/s\u001b[0m  \u001b[33m0:00:14\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.3-cp310-cp310-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m587.2 kB/s\u001b[0m  \u001b[33m0:00:20\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (14.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m624.7 kB/s\u001b[0m  \u001b[33m0:00:22\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (35.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.5/35.5 MB\u001b[0m \u001b[31m512.7 kB/s\u001b[0m  \u001b[33m0:01:20\u001b[0mm0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, numpy, joblib, scipy, pandas, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.3 numpy-2.2.6 pandas-2.3.3 pytz-2025.2 scikit-learn-1.7.2 scipy-1.15.3 threadpoolctl-3.6.0 tzdata-2025.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "### install scikit-learn and pandas\n",
    "!pip install scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5487b49f-fba4-4307-a01e-ae159879e655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               apel  berwarna   dimakan       itu     merah     sudah  \\\n",
      "Dokumen A  0.449436  0.631667  0.000000  0.000000  0.631667  0.000000   \n",
      "Dokumen B  0.335176  0.000000  0.471078  0.471078  0.000000  0.471078   \n",
      "\n",
      "               ulat  \n",
      "Dokumen A  0.000000  \n",
      "Dokumen B  0.471078  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. Setup data (korspus)\n",
    "# Dokumen A (Index 0) : \"Apel berwarna merah\"\n",
    "# Dokumen B (Index 1) : \"Apel itu sudah dimakan ulat\"\n",
    "\n",
    "corpus = [\n",
    "    \"Apel berwarna merah\",\n",
    "    \"Apel itu sudah dimakan ulat\"\n",
    "]\n",
    "\n",
    "# 2. Initialize Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# down here will start calculate using TF-IDF\n",
    "# 3. Calculate TF-IDF (Fit & Transform)\n",
    "# Machine will calculate (Fit) the value in docs\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 4. Show result\n",
    "# get name \"word\" (feature) from machine\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# create table\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names, index=[\"Dokumen A\", \"Dokumen B\"])\n",
    "\n",
    "\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e5fd25-0715-4af6-822a-58e6721d5754",
   "metadata": {},
   "source": [
    "### Kata Apel itu mendapatakan nilai sekitar 0.4 dan 0.3 tidak terlalu tinggi nilainya (jika dilihat per dokumen) itu karna kata tersebut muncul di kedua dokumen, lalu ada kata bewarna merah di dok A sebagai \"ciri khas\" dokumen tersebut, sedangkan di dokumen B ada kata dimakan, itu, sudah sebagai ciri khas dokumen B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
