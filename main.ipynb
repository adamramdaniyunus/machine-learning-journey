{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07442c6-9dd1-4d1b-9cb2-aadfd6616fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"\n",
    "    Di sebuah desa yang terkenal dengan hasil pertaniannya,\n",
    "    para petani menanam berbagai jenis buah seperti pisang, jeruk, dan apel.\n",
    "    Setiap musim panen tiba, apel menjadi buah yang paling banyak dicari karena rasanya\n",
    "    yang segar dan kandungan gizinya yang tinggi. Tidak hanya dijual dalam bentuk buah segar,\n",
    "    apel juga diolah menjadi jus, selai, dan kue oleh para pelaku UMKM setempat. Menariknya,\n",
    "    beberapa petani mulai bereksperimen dengan varietas apel baru yang lebih tahan terhadap perubahan cuaca.\n",
    "    Walaupun harga apel kadang naik turun di pasar, permintaan terhadap apel tetap stabil\n",
    "    karena masyarakat sudah terbiasa mengonsumsi Apeljack sebagai bagian dari gaya hidup sehat.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e48dd9-9c0a-425a-9c31-52f06aa8e091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# count word \"Apel\" on paragraph using count \n",
    "# but when text include \"Apeljack\" it will still count\n",
    "\n",
    "wordApel = paragraph.lower().count(\"apel\")\n",
    "print(wordApel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e86db0e-69ab-4429-be22-2157818e9ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# count word \"Apel\" on pargraph using counter and regex\n",
    "import re\n",
    "\n",
    "wordApel = sum(1 for match in re.finditer(r\"\\bapel\\b\", paragraph.lower()))\n",
    "\n",
    "print(wordApel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b387076-8fe2-499a-a0c5-0fb6570dfa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# count word \"Apel\" on paragraph using counter and collections\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "wordApel = re.findall(r'\\b\\w+\\b', paragraph.lower())\n",
    "counter = Counter(wordApel)\n",
    "\n",
    "print(counter[\"apel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e00bdb4",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8de2e4-f5cb-4ac7-9172-a68efa2b9386",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "is a method for evaluating how important a word is in a documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "411cc05b-7a00-44ed-b8cc-3ef31f721464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp310-cp310-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.3-cp310-cp310-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl.metadata (91 kB)\n",
      "Collecting numpy>=1.22.0 (from scikit-learn)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (63 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached scikit_learn-1.7.2-cp310-cp310-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl (9.5 MB)\n",
      "Using cached pandas-2.3.3-cp310-cp310-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl (12.1 MB)\n",
      "Using cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (14.3 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (35.5 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, numpy, joblib, scipy, pandas, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.3 numpy-2.2.6 pandas-2.3.3 pytz-2025.2 scikit-learn-1.7.2 scipy-1.15.3 threadpoolctl-3.6.0 tzdata-2025.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "### install scikit-learn and pandas\n",
    "!pip install scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487b49f-fba4-4307-a01e-ae159879e655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               apel  berwarna   dimakan       itu     merah     sudah  \\\n",
      "Dokumen A  0.449436  0.631667  0.000000  0.000000  0.631667  0.000000   \n",
      "Dokumen B  0.335176  0.000000  0.471078  0.471078  0.000000  0.471078   \n",
      "\n",
      "               ulat  \n",
      "Dokumen A  0.000000  \n",
      "Dokumen B  0.471078  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. Setup data (korspus)\n",
    "# Dokumen A (Index 0) : \"Apel berwarna merah\"\n",
    "# Dokumen B (Index 1) : \"Apel itu sudah dimakan ulat\"\n",
    "\n",
    "corpus = [\n",
    "    \"Apel berwarna merah\",\n",
    "    \"Apel itu sudah dimakan ulat\"\n",
    "]\n",
    "\n",
    "# 2. Initialize Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# down here will start calculate using TF-IDF\n",
    "# 3. Calculate TF-IDF (Fit & Transform)\n",
    "# Machine will calculate (Fit) the value in docs\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 4. Show result\n",
    "# get name \"word\" (feature) from machine\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# create table\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names, index=[\"Dokumen A\", \"Dokumen B\"])\n",
    "\n",
    "\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e5fd25-0715-4af6-822a-58e6721d5754",
   "metadata": {},
   "source": [
    "Kata Apel itu mendapatakan nilai sekitar 0.4 dan 0.3 tidak terlalu tinggi nilainya (jika dilihat per dokumen) itu karna kata tersebut muncul di kedua dokumen, lalu ada kata bewarna merah di dok A sebagai \"ciri khas\" dokumen tersebut, sedangkan di dokumen B ada kata dimakan, itu, sudah sebagai ciri khas dokumen B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe0a70-ef92-4c0a-a1eb-cf65a85f9dae",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e576e77-29e9-4f4f-a874-992fb7b45f2b",
   "metadata": {},
   "source": [
    "Sebuah metode untuk mengukur seberapa mirip dua benda (dalam hal ini dokument/teks) dengan menghitung kosinus sudut di antara kedua vektor dokumen tersebut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938fda44-f618-427c-bfaa-c864b8172c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7999999999999998\n"
     ]
    }
   ],
   "source": [
    "## implement\n",
    "# first using numpy\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# 1. vektor\n",
    "# Dokumen A: 1 Apel, 2 Manis\n",
    "\n",
    "vec_a = np.array([1,2])\n",
    "\n",
    "# Dokumen B: 2 Apel, 1 Manis\n",
    "\n",
    "vec_b = np.array([2,1])\n",
    "\n",
    "# 2. Calculate Dot Product\n",
    "# Rumus: (1*2) + (2*1) = 4\n",
    "\n",
    "dot_product = np.dot(vec_a, vec_b)\n",
    "\n",
    "# 3. Calculate (Magnitude / Length Vector)\n",
    "# Rumus: Akar ( ||A|| * ||B||)\n",
    "\n",
    "norm_a = norm(vec_a)\n",
    "norm_b = norm(vec_b)\n",
    "\n",
    "# 4. Hitung Cosine Similarity\n",
    "# Rumus: dot_product / (norm_a * norm_b)\n",
    "\n",
    "result = dot_product / (norm_a * norm_b)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa95d1-43db-4008-be0b-1fae0930cf8e",
   "metadata": {},
   "source": [
    "Hasilnya 0.799... itu berarti dokumen A dan dokumen B memiliki kemiripan sekitar 79 - 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bec30d-ff9f-49d9-aff2-0674fd113611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil Cosine similarity : 0.7999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Using scikit-learn\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data \n",
    "vec_a = np.array([[1,2]])\n",
    "vec_b = np.array([[2,1]])\n",
    "\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity = cosine_similarity(vec_a, vec_b)\n",
    "\n",
    "print(\"Hasil Cosine similarity :\", similarity[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1e2fa-cf32-431d-ae9d-a86cd28953dc",
   "metadata": {},
   "source": [
    "Hasilnya samaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf5581-1da2-4d11-aa83-be8eb6594a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp310-cp310-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp310-cp310-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl.metadata (91 kB)\n",
      "Collecting numpy>=1.22.0 (from scikit-learn)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (63 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading scikit_learn-1.7.2-cp310-cp310-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.3-cp310-cp310-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (14.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (35.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.5/35.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, numpy, joblib, scipy, pandas, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [scikit-learn][0m [scikit-learn]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.3 numpy-2.2.6 pandas-2.3.3 pytz-2025.2 scikit-learn-1.7.2 scipy-1.15.3 threadpoolctl-3.6.0 tzdata-2025.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c43f6-f08a-4907-b9d0-3d2e46a07c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting Sastrawi\n",
      "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/nlp/lib/python3.10/site-packages (from nltk) (1.5.3)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2026.1.15-cp310-cp310-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
      "Downloading regex-2026.1.15-cp310-cp310-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: Sastrawi, tqdm, regex, click, nltk\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [nltk][32m4/5\u001b[0m [nltk]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Sastrawi-1.0.1 click-8.3.1 nltk-3.9.2 regex-2026.1.15 tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ff494-7289-4bcd-9b43-b730457c9b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: B\n",
      "Similarity: 0.17662402681611103\n",
      "Result: Pakta Bentukan Arab Saudi Rombak Aliansi Keamanan Timur Tengah?\n",
      "\n",
      "Query: B\n",
      "Similarity: 0.19568723861021348\n",
      "Result: Reza Arap Penuhi Panggilan Polisi soal Lula Lahfah, Ditanya 30 Pertanyaan\n",
      "\n",
      "Query: B\n",
      "Similarity: 0.19568723861021348\n",
      "Result: Reza Arap Penuhi Panggilan Polisi soal Lula Lahfah, Ditanya 30 Pertanyaan\n",
      "\n",
      "[\"Eks Anggota DPRD Bekasi F-PDIP Dipanggil KPK Terkait Kasus Ade Kuswara\", \"KPK Panggil Direktur Pemeriksaan-Penagihan DJP Terkait Kasus Suap Pajak Jakut\", \"Permukiman Liar Dibongkar, TPU Kebon Nanas Siap Tampung Ribuan Makam Baru\", \"Video: Sah! Thomas Djiwandono Jadi Deputi Gubernur BI\", \"Indeks Perkembangan Harga di Aceh, Sumbar & Sumut Turun Signifikan\", \"Eks Anggota DPRD Bekasi F-PDIP Dipanggil KPK Terkait Kasus Ade Kuswara\", \"KPK Panggil Direktur Pemeriksaan-Penagihan DJP Terkait Kasus Suap Pajak Jakut\", \"Lansia di Lebak Tenggelam Saat Coba Seberangi Sungai Pakai Batang Pisang\", \"Komisi I DPR Gelar Rapat Tertutup Bareng Menlu Sugiono\", \"Video Gedung Putih Tuding Demokrat Jadi Dalang Kerusuhan di Minneapolis\", \"Rocky Gerung Diperiksa di Kasus Ijazah Jokowi, Jelaskan Hal Ini ke Polisi\", \"Konsolidasi Kekuasaan To Lam, Taruhan Politik Vietnam\", \"BP MPR Gelar Rapim, Tetapkan 5 Topik Kajian 2026 & Program Kegiatan Baru\", \"Sekjen NATO: Eropa Tak Siap Bertahan Tanpa AS\", \"HNW Ingatkan Partisipasi RI dalam Dewan Perdamaian Harus Taat Konstitusi\", \"Pesawat Smart Air Jatuh di Pantai Nabire, Semua Penumpang Selamat\", \"Jadi Deputi Gubernur BI Terpilih, Tommy Djiwandono Tegaskan Jaga Independensi\", \"Hujan Salju Bikin 7.000 Orang Terjebak di Bandara Jepang\", \"Waka MPR Dorong Upaya Wujudkan Keluarga Jadi Ruang Aman bagi Perempuan\", \"Budi Djiwandono Respons Seloroh Utut soal Akan Masuk Eksekutif\", \"Polantas di Riau Masuk SD, Edukasi Green Policing dan Aman Berlalu Lintas\", \"Habiburokhman soal Adies Jadi Calon Hakim MK: Dia Nggak Sakiti Siapa Pun\", \"Video: Detik-detik Wali Kota Filipina Lolos dari Serangan RPG\", \"Wali Kota Metro Siap Sediakan Lahan Sekolah Rakyat Permanen\", \"Ahok di Sidang Kasus Minyak: Golf Tempat Negosiasi Paling Murah\", \"Video Rocky Gerung Sebut Ijazah Jokowi Asli: Tapi Orangnya yang Palsu\", \"Pramono Minta RS Jakarta Melayani Pakai Hati: BPJS dan Non-BPJS Sama\", \"I Wayan Sudirta Gantikan TB Hasanuddin Jadi Wakil Ketua MKD DPR\", \"Pantai Utara Tegal Dipenuhi Kayu Gelondongan Usai Banjir Bandang\", \"Sejalan Arahan Presiden, Pekanbaru Terapkan Manajemen Talenta ASN\", \"187 Ton Sampah Usai Banjir Diangkut dari Pinggir Kali di Jakbar\", \"Video: Penembakan Maut di Lapangan Sepak Bola Meksiko, 11 Orang Tewas\", \"BNN Wanti-wanti soal N2O 'Gas Tertawa', Bisa Menyebabkan Kematian\", \"Pemulihan Pascabencana Aceh, 70 Relawan BRI Peduli Bersih-bersih Sekolah\", \"Pakta Bentukan Arab Saudi Rombak Aliansi Keamanan Timur Tengah?\", \"Terjadi Lagi, Pelat Besi JPO di Daan Mogot Jakbar Hilang Dicuri\", \"Paripurna Tetapkan Polri di Bawah Presiden Jadi Keputusan Mengikat DPR-Pemerintah\", \"Pemkot Depok Terapkan WFH bagi ASN Setiap Kamis, Pelayanan Publik Tetap Normal\", \"Ibu di Kalbar Cabuli Anak Angkat Laki-laki, Rekam Aksi untuk Dipamerkan\", \"Pelindo Dukung Pengembangan Kawasan Pesisir-Pelabuhan Laut di Jakarta\", \"Video Rocky Gerung Jadi Saksi Roy Suryo di Kasus Ijazah Jokowi\", \"30 Orang Tewas Akibat Badai Musim Dingin di AS\", \"Potret Ahok Bersaksi di Sidang Kasus Tata Niaga Minyak\", \"Video Paripurna DPR Sahkan Adies Kadir Calon Hakim MK Usulan DPR\", \"Komisi III DPR Ungkap Alasan Adies Calon Hakim MK Gantikan Inosentius\", \"Feminisme Pesantren: Membangun Tradisi Adil dari Dalam\", \"Menanti Huntara, Penyintas Banjir Maninjau Bertahan di Bekas Kandang Sapi\", \"Reza Arap Penuhi Panggilan Polisi soal Lula Lahfah, Ditanya 30 Pertanyaan\", \"Profil Sari Yuliati, Bendum Golkar Kini Jabat Wakil Ketua DPR RI\", \"Kembangkan Penegakan Hukum Digital, Kakorlantas Terus Sempurnakan ETLE\", \"6 WNI Dihukum Bui-Cambuk Gegara Masuk Singapura secara Ilegal\", \"Video Permintaan Maaf Aparat yang Curigai Es Kue dari Bahan Spons\", \"prabowo subianto\", \"banjir\", \"lula lahfah\", \"Reza Arap Penuhi Panggilan Polisi soal Lula Lahfah, Ditanya 30 Pertanyaan\", \"Ahok di Sidang Kasus Minyak: Golf Tempat Negosiasi Paling Murah\", \"Rocky Gerung Diperiksa di Kasus Ijazah Jokowi, Jelaskan Hal Ini ke Polisi\", \"Pesawat Smart Air Jatuh di Pantai Nabire, Semua Penumpang Selamat\", \"Sari Yuliati Golkar Gantikan Adies Kadir Jadi Wakil Ketua DPR\", \"Kepala BNN hingga Mendes Launching Nasional Indonesia Bersinar, Dukung Asta Cita\", \"BNN Perkuat Sinergi dengan CNB Singapura untuk Lacak TPPU Bandar Narkoba\", \"BNN-Polda Sumut Gerebek Kampung Narkoba di Deli Serdang, Tangkap 19 Orang\"]\n"
     ]
    }
   ],
   "source": [
    "## Exercise\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "\n",
    "list_data = []\n",
    "\n",
    "with open(\"data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    list_data = json.load(f)\n",
    "\n",
    "# implementasi cosine similarity\n",
    "\n",
    "# implement stemming\n",
    "# using nltk and Sastrawi\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# Using stop word better query\n",
    "factory_sw = StopWordRemoverFactory()\n",
    "stopwords = factory_sw.get_stop_words()\n",
    "\n",
    "\n",
    "# Init dlu yang mau di stopwords : gabungkan dengan yg default\n",
    "custom_stopwords = stopwords + [\"kasus\", \"diduga\", \"terkait\", \"anggota\"]\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    words = text.split()\n",
    "\n",
    "    filterred_words = [w for w in words if w not in custom_stopwords]\n",
    "    text = \" \".join(filterred_words)\n",
    "    text = stemmer.stem(text)\n",
    "    return text\n",
    "\n",
    "query = \"Berita kasus reza arab\"\n",
    "query_stemmed = preprocess(query)\n",
    "\n",
    "\n",
    "# include to vectorizer\n",
    "\n",
    "# masih ada issue misalnya ketik reza arab tpi data \"arab\" juga muncul\n",
    "# ini tidak relevan dengan querynya\n",
    "# solusinya? pakai n-grams\n",
    "# ngram_range=(3, 5): Baca potongan 3 huruf sampai 5 huruf\n",
    "# syntaxnya mirip slice di js :v\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(3, 5))\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "clean_data = []\n",
    "for doc in list_data:\n",
    "    clean_data.append(preprocess(doc))\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(clean_data)\n",
    "\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "\n",
    "# Create vector\n",
    "vector_query = vectorizer.transform([query])\n",
    "\n",
    "# calculate cosine similarity\n",
    "similarity = cosine_similarity(vector_query, tfidf_matrix)\n",
    "\n",
    "results = similarity[0]\n",
    "\n",
    "for i, x in enumerate(results):\n",
    "    if x >= 0.1:\n",
    "        print(\"Query:\", query[0])\n",
    "        print(\"Similarity:\", x)\n",
    "        print(\"Result:\", list_data[i])\n",
    "        print()\n",
    "\n",
    "print(json.dumps(list_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae146cc-dd73-4e57-ac1d-059b8dc7aaa8",
   "metadata": {},
   "source": [
    "## Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe51b7d-c6eb-4b19-83ec-b6dd531fdb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.2.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp310-cp310-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp310-cp310-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl.metadata (91 kB)\n",
      "Collecting transformers<6.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-5.0.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-1.3.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.10.0-cp310-cp310-manylinux_2_28_aarch64.whl.metadata (31 kB)\n",
      "Collecting numpy (from sentence-transformers)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (63 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from sentence-transformers) (4.15.0)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting filelock (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2026.1.15-cp310-cp310-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (7.3 kB)\n",
      "Collecting typer-slim (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (4.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading fsspec-2026.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_28_aarch64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
      "Collecting shellingham (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: anyio in /opt/conda/envs/nlp/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.10.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/nlp/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/envs/nlp/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/envs/nlp/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Collecting click>=8.0.0 (from typer-slim->transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading sentence_transformers-5.2.2-py3-none-any.whl (494 kB)\n",
      "Downloading transformers-5.0.0-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m60.6 kB/s\u001b[0m  \u001b[33m0:03:10\u001b[0mm0:00:03\u001b[0m0:07\u001b[0mm\n",
      "\u001b[?25hDownloading huggingface_hub-1.3.5-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m46.3 kB/s\u001b[0m  \u001b[33m0:00:09\u001b[0m6m-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_28_aarch64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m93.5 kB/s\u001b[0m  \u001b[33m0:00:46\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/3.3 MB\u001b[0m \u001b[31m134.2 kB/s\u001b[0m eta \u001b[36m0:00:11\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3445727-2c8b-4ccc-9913-d0602f90b3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45c7d3e07fc403fbc8be5ed9753997a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: kasus orang mati\n",
      "------------------------------\n",
      "                                              Dokumen      Skor\n",
      "33  Pemulihan Pascabencana Aceh, 70 Relawan BRI Pe...  0.492815\n",
      "32  BNN Wanti-wanti soal N2O 'Gas Tertawa', Bisa M...  0.474350\n",
      "31  Video: Penembakan Maut di Lapangan Sepak Bola ...  0.429450\n",
      "2   Permukiman Liar Dibongkar, TPU Kebon Nanas Sia...  0.408055\n",
      "41     30 Orang Tewas Akibat Badai Musim Dingin di AS  0.398150\n",
      "..                                                ...       ...\n",
      "34  Pakta Bentukan Arab Saudi Rombak Aliansi Keama...  0.043246\n",
      "39  Pelindo Dukung Pengembangan Kawasan Pesisir-Pe...  0.032999\n",
      "61  BNN Perkuat Sinergi dengan CNB Singapura untuk...  0.031005\n",
      "23  Wali Kota Metro Siap Sediakan Lahan Sekolah Ra...  0.025947\n",
      "37  Pemkot Depok Terapkan WFH bagi ASN Setiap Kami... -0.032137\n",
      "\n",
      "[63 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# load model\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "\n",
    "# Load data\n",
    "list_data = []\n",
    "with open(\"data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    list_data = json.load(f)\n",
    "\n",
    "# Convert data to vector\n",
    "doc_embeddings = model.encode(list_data)\n",
    "\n",
    "\n",
    "query = \"kasus orang mati\"\n",
    "\n",
    "# Encode query\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "\n",
    "similarity = cosine_similarity(query_embedding, doc_embeddings)\n",
    "\n",
    "# results\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "df = pd.DataFrame({'Dokumen': list_data, 'Skor': similarity[0]})\n",
    "print(df.sort_values(by='Skor', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac3dc4-bbdd-4800-b310-8949f761d9aa",
   "metadata": {},
   "source": [
    "## Study Case TPA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a91fd3-0d33-4570-90f7-e1ac46c24310",
   "metadata": {},
   "source": [
    "disini saya ingin cobat terapkan beberapa materi yang sudah dipelajari kedalam kasus PRE-AUTH AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a90d74-3181-4f45-92d7-e3b54559021e",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ed973-e2de-4bc1-b99d-349956e6cbd7",
   "metadata": {},
   "source": [
    "kerangka kerja AI yang membantu LLM menghadirkan respons yang lebih akurat dan relevan dengan mengizinkan model mengakses data yang tidak disertakan dalam pelatihan mereka.\n",
    "\n",
    "- R (Retrieval) - PENCARIAN\n",
    "- A (Augmented) - PENGGABUNGAN\n",
    "- G (Generation) - PEMBUATAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5680b27c-a929-43a8-82d9-15e3aed5a23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.2.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/nlp/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/nlp/lib/python3.10/site-packages (2.3.3)\n",
      "Collecting transformers<6.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-5.0.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-1.3.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.10.0-cp310-cp310-manylinux_2_28_aarch64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/nlp/lib/python3.10/site-packages (from sentence-transformers) (2.2.6)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/nlp/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from sentence-transformers) (4.15.0)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting filelock (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached regex-2026.1.15-cp310-cp310-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.22.2-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (7.3 kB)\n",
      "Collecting typer-slim (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (4.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached fsspec-2026.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_28_aarch64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
      "Collecting shellingham (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: anyio in /opt/conda/envs/nlp/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.10.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/nlp/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/envs/nlp/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/envs/nlp/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/nlp/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Collecting click>=8.0.0 (from typer-slim->transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Using cached sentence_transformers-5.2.2-py3-none-any.whl (494 kB)\n",
      "Using cached transformers-5.0.0-py3-none-any.whl (10.1 MB)\n",
      "Using cached huggingface_hub-1.3.5-py3-none-any.whl (536 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_28_aarch64.whl (3.2 MB)\n",
      "Using cached tokenizers-0.22.2-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (3.3 MB)\n",
      "Using cached fsspec-2026.1.0-py3-none-any.whl (201 kB)\n",
      "Using cached regex-2026.1.15-cp310-cp310-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (781 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (491 kB)\n",
      "Downloading torch-2.10.0-cp310-cp310-manylinux_2_28_aarch64.whl (146.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m  \u001b[33m0:00:15\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m36m-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Installing collected packages: mpmath, tqdm, sympy, shellingham, safetensors, regex, networkx, hf-xet, fsspec, filelock, click, typer-slim, torch, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/17\u001b[0m [sentence-transformers]ence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed click-8.3.1 filelock-3.20.3 fsspec-2026.1.0 hf-xet-1.2.0 huggingface-hub-1.3.5 mpmath-1.3.0 networkx-3.4.2 regex-2026.1.15 safetensors-0.7.0 sentence-transformers-5.2.2 shellingham-1.5.4 sympy-1.14.0 tokenizers-0.22.2 torch-2.10.0 tqdm-4.67.1 transformers-5.0.0 typer-slim-0.21.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c03b163-3bcb-4035-b2a0-2efc8b5f41f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'preauthId': 'PREAUTH-001', 'preauthNumber': 'PA-2026-0001', 'hospital': {'provider_name': 'RS Sehat Sentosa', 'name': 'Budi Santoso', 'age': 45, 'gender': 'MALE', 'request_date': '2026-01-29', 'admission_type': 'In-Patient', 'primary_icd10': 'I10', 'primary_icd10_description': 'Essential (primary) hypertension', 'procedures': 'Pemeriksaan laboratorium dan observasi tekanan darah', 'clinical_justification': 'Pasien mengalami tekanan darah tinggi disertai pusing dan nyeri kepala.', 'urgency_level': 'URGENT', 'scheduled_date': '2026-02-01', 'estimated_length_of_stay': 3, 'planned_procedures': [{'procedure_code': 'LAB001', 'description': 'Pemeriksaan darah lengkap'}, {'procedure_code': 'OBS001', 'description': 'Observasi tekanan darah'}], 'estimated_cost': [{'item': 'Biaya Rawat Inap', 'description': 'Kamar kelas 1', 'amount': 1500000}, {'item': 'Biaya Laboratorium', 'description': 'Cek darah lengkap', 'amount': 500000}], 'documents': [{'type': 'Resume Medis', 'file_name': 'resume_medis_budi.pdf', 'url': 'https://dummy-storage/documents/resume_medis_budi.pdf'}, {'type': 'Dokumen Pendukung', 'file_name': 'hasil_lab.pdf', 'url': 'https://dummy-storage/documents/hasil_lab.pdf'}, {'type': 'Foto Copy Kartu Peserta', 'file_name': 'kartu_peserta.jpg', 'url': 'https://dummy-storage/documents/kartu_peserta.jpg'}, {'type': 'Foto Copy Surat Jaminan', 'file_name': 'surat_jaminan.pdf', 'url': 'https://dummy-storage/documents/surat_jaminan.pdf'}]}, 'insurance': {'name': 'Budi Santoso', 'age': 45, 'gender': 'MALE', 'contract_date_start': '2024-01-01', 'contract_date_end': '2026-12-31', 'coverage_details': 'Asuransi Kesehatan Gold Plan', 'age_limit': 65, 'annual_limit': 50000000, 'remaining_coverage': 32000000, 'plan_benefits': [{'benefit_name': 'Rawat Inap', 'limit': 20000000}, {'benefit_name': 'Laboratorium', 'limit': 5000000}], 'waiting_period': ['30 hari untuk rawat inap', '90 hari untuk penyakit tertentu'], 'exclusions': ['Penyakit bawaan sejak lahir'], 'pre_existing_conditions': ['Hipertensi'], 'previous_claims': [{'claim_id': 'CLM-2025-001', 'claim_date': '2025-08-12', 'claim_type': 'Hospitalization', 'claim_amount': 3500000, 'claim_paid': 3500000, 'status': 'PAID', 'provider_name': 'RS Sehat Sentosa'}]}}\n"
     ]
    }
   ],
   "source": [
    "## Study Case\n",
    "### TPA Classification Pre-Auth\n",
    "\n",
    "# this is payload from pre-auth now\n",
    "\n",
    "dummyPayload = {\n",
    "    \"preauthId\": \"PREAUTH-001\",\n",
    "    \"preauthNumber\": \"PA-2026-0001\",\n",
    "    \"hospital\": {\n",
    "        \"provider_name\": \"RS Sehat Sentosa\",\n",
    "        \"name\": \"Budi Santoso\",\n",
    "        \"age\": 45,\n",
    "        \"gender\": \"MALE\",\n",
    "        \"request_date\": \"2026-01-29\",\n",
    "        \"admission_type\": \"In-Patient\",\n",
    "        \"primary_icd10\": \"I10\",\n",
    "        \"primary_icd10_description\": \"Essential (primary) hypertension\",\n",
    "        \"procedures\": \"Pemeriksaan laboratorium dan observasi tekanan darah\",\n",
    "        \"clinical_justification\": \"Pasien mengalami tekanan darah tinggi disertai pusing dan nyeri kepala.\",\n",
    "        \"urgency_level\": \"URGENT\",\n",
    "        \"scheduled_date\": \"2026-02-01\",\n",
    "        \"estimated_length_of_stay\": 3,\n",
    "        \"planned_procedures\": [\n",
    "            {\n",
    "                \"procedure_code\": \"LAB001\",\n",
    "                \"description\": \"Pemeriksaan darah lengkap\",\n",
    "            },\n",
    "            {\n",
    "                \"procedure_code\": \"OBS001\",\n",
    "                \"description\": \"Observasi tekanan darah\",\n",
    "            },\n",
    "        ],\n",
    "        \"estimated_cost\": [\n",
    "            {\n",
    "                \"item\": \"Biaya Rawat Inap\",\n",
    "                \"description\": \"Kamar kelas 1\",\n",
    "                \"amount\": 1_500_000,\n",
    "            },\n",
    "            {\n",
    "                \"item\": \"Biaya Laboratorium\",\n",
    "                \"description\": \"Cek darah lengkap\",\n",
    "                \"amount\": 500_000,\n",
    "            },\n",
    "        ],\n",
    "        \"documents\": [\n",
    "            {\n",
    "                \"type\": \"Resume Medis\",\n",
    "                \"file_name\": \"resume_medis_budi.pdf\",\n",
    "                \"url\": \"https://dummy-storage/documents/resume_medis_budi.pdf\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"Dokumen Pendukung\",\n",
    "                \"file_name\": \"hasil_lab.pdf\",\n",
    "                \"url\": \"https://dummy-storage/documents/hasil_lab.pdf\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"Foto Copy Kartu Peserta\",\n",
    "                \"file_name\": \"kartu_peserta.jpg\",\n",
    "                \"url\": \"https://dummy-storage/documents/kartu_peserta.jpg\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"Foto Copy Surat Jaminan\",\n",
    "                \"file_name\": \"surat_jaminan.pdf\",\n",
    "                \"url\": \"https://dummy-storage/documents/surat_jaminan.pdf\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    \"insurance\": {\n",
    "        \"name\": \"Budi Santoso\",\n",
    "        \"age\": 45,\n",
    "        \"gender\": \"MALE\",\n",
    "        \"contract_date_start\": \"2024-01-01\",\n",
    "        \"contract_date_end\": \"2026-12-31\",\n",
    "        \"coverage_details\": \"Asuransi Kesehatan Gold Plan\",\n",
    "        \"age_limit\": 65,\n",
    "        \"annual_limit\": 50_000_000,\n",
    "        \"remaining_coverage\": 32_000_000,\n",
    "        \"plan_benefits\": [\n",
    "            {\n",
    "                \"benefit_name\": \"Rawat Inap\",\n",
    "                \"limit\": 20_000_000,\n",
    "            },\n",
    "            {\n",
    "                \"benefit_name\": \"Laboratorium\",\n",
    "                \"limit\": 5_000_000,\n",
    "            },\n",
    "        ],\n",
    "        \"waiting_period\": [\n",
    "            \"30 hari untuk rawat inap\",\n",
    "            \"90 hari untuk penyakit tertentu\",\n",
    "        ],\n",
    "        \"exclusions\": [\n",
    "            \"Penyakit bawaan sejak lahir\",\n",
    "        ],\n",
    "        \"pre_existing_conditions\": [\n",
    "            \"Hipertensi\",\n",
    "        ],\n",
    "        \"previous_claims\": [\n",
    "            {\n",
    "                \"claim_id\": \"CLM-2025-001\",\n",
    "                \"claim_date\": \"2025-08-12\",\n",
    "                \"claim_type\": \"Hospitalization\",\n",
    "                \"claim_amount\": 3_500_000,\n",
    "                \"claim_paid\": 3_500_000,\n",
    "                \"status\": \"PAID\",\n",
    "                \"provider_name\": \"RS Sehat Sentosa\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "print(dummyPayload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33eddd80-589f-4fe2-85d4-0e6d646aacbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adcc9ef24d874661ade658dbad4a6e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c76eb9f22541c5b923a4436b738a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423ac0678469469db67a09cda2aee9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3851ea324fb47b4b08dd731a7487595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56de0ca6e82c495b8d70cfd1483fa52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ae2bc0242f409ca635a237eb91a0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 5] Input/output error: '/root/.cache/huggingface/hub/models--BAAI--bge-m3/refs/main'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/utils/hub.py:419\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:89\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_legacy_arguments(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1024\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, etag_timeout, token, local_files_only, headers, endpoint, tqdm_class, dry_run)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdry_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1240\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, force_download, tqdm_class, dry_run)\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1240\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1864\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, headers, expected_size, filename, force_download, etag, xet_file_data, tqdm_class)\u001b[0m\n\u001b[1;32m   1863\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1864\u001b[0m     \u001b[43mxet_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mincomplete_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/huggingface_hub/file_download.py:588\u001b[0m, in \u001b[0;36mxet_get\u001b[0;34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, tqdm_class, _tqdm_bar)\u001b[0m\n\u001b[1;32m    586\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(progress_bytes)\n\u001b[0;32m--> 588\u001b[0m \u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxet_download_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpiration_unix_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Data processing error: CAS service error : IO Error: Input/output error (os error 5)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBAAI/bge-m3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# dummy database\u001b[39;00m\n\u001b[1;32m     15\u001b[0m db_icd10 \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI10 - Essential (primary) hypertension (Darah Tinggi)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE11 - Type 2 diabetes mellitus (Kencing Manis)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA01 - Typhoid fever (Tipes)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:327\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    309\u001b[0m has_modules \u001b[38;5;241m=\u001b[39m is_sentence_transformer_model(\n\u001b[1;32m    310\u001b[0m     model_name_or_path,\n\u001b[1;32m    311\u001b[0m     token,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    315\u001b[0m )\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    317\u001b[0m     has_modules\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model_type(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    326\u001b[0m ):\n\u001b[0;32m--> 327\u001b[0m     modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    339\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[1;32m    340\u001b[0m         model_name_or_path,\n\u001b[1;32m    341\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m         has_modules\u001b[38;5;241m=\u001b[39mhas_modules,\n\u001b[1;32m    350\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:2305\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   2300\u001b[0m         module \u001b[38;5;241m=\u001b[39m module_class\u001b[38;5;241m.\u001b[39mload(local_path)\n\u001b[1;32m   2302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2303\u001b[0m     \u001b[38;5;66;03m# Newer modules that support the new loading method are loaded with the new style\u001b[39;00m\n\u001b[1;32m   2304\u001b[0m     \u001b[38;5;66;03m# i.e. with many keyword arguments that can optionally be used by the modules\u001b[39;00m\n\u001b[0;32m-> 2305\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2307\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Loading-specific keyword arguments\u001b[39;49;00m\n\u001b[1;32m   2308\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2313\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Module-specific keyword arguments\u001b[39;49;00m\n\u001b[1;32m   2314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2319\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2321\u001b[0m modules[module_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m   2322\u001b[0m module_kwargs[module_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m module_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:366\u001b[0m, in \u001b[0;36mTransformer.load\u001b[0;34m(cls, model_name_or_path, subfolder, token, cache_folder, revision, local_files_only, trust_remote_code, model_kwargs, tokenizer_kwargs, config_kwargs, backend, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    352\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m    353\u001b[0m     init_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_init_kwargs(\n\u001b[1;32m    354\u001b[0m         model_name_or_path\u001b[38;5;241m=\u001b[39mmodel_name_or_path,\n\u001b[1;32m    355\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m         backend\u001b[38;5;241m=\u001b[39mbackend,\n\u001b[1;32m    365\u001b[0m     )\n\u001b[0;32m--> 366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:89\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[0m\n\u001b[1;32m     86\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     88\u001b[0m config, is_peft_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_peft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Get the signature of the auto_model's forward method to pass only the expected arguments from `features`,\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# plus some common values like \"input_ids\", \"attention_mask\", etc.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m model_forward_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model\u001b[38;5;241m.\u001b[39mforward)\u001b[38;5;241m.\u001b[39mparameters)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:197\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m load_onnx_model(\n\u001b[1;32m    202\u001b[0m         model_name_or_path\u001b[38;5;241m=\u001b[39mmodel_name_or_path,\n\u001b[1;32m    203\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    204\u001b[0m         task_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature-extraction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:372\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/modeling_utils.py:4038\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/modeling_utils.py:668\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, variant, gguf_file, use_safetensors, user_agent, is_remote_code, transformers_explicit_filename, download_kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/utils/hub.py:276\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    222\u001b[0m     path_or_repo_id: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike,\n\u001b[1;32m    223\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    225\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/utils/hub.py:471\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[0;32m--> 471\u001b[0m resolved_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    472\u001b[0m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[1;32m    474\u001b[0m ]\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_files\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/utils/hub.py:472\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[1;32m    471\u001b[0m resolved_files \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 472\u001b[0m     \u001b[43m_get_cache_file_to_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[1;32m    474\u001b[0m ]\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_files\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/utils/hub.py:107\u001b[0m, in \u001b[0;36m_get_cache_file_to_return\u001b[0;34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_cache_file_to_return\u001b[39m(\n\u001b[1;32m    100\u001b[0m     path_or_repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    101\u001b[0m     full_filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m ):\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# We try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mtry_to_load_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resolved_file \u001b[38;5;241m!=\u001b[39m _CACHED_NO_EXIST:\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:89\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         validate_repo_id(arg_value)\n\u001b[1;32m     87\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_legacy_arguments(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1541\u001b[0m, in \u001b[0;36mtry_to_load_from_cache\u001b[0;34m(repo_id, filename, cache_dir, revision, repo_type)\u001b[0m\n\u001b[1;32m   1539\u001b[0m     revision_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(refs_dir, revision)\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(revision_file):\n\u001b[0;32m-> 1541\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrevision_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m   1542\u001b[0m             revision \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;66;03m# Check if file is cached as \"no_exist\"\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: '/root/.cache/huggingface/hub/models--BAAI--bge-m3/refs/main'"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    \"BAAI/bge-m3\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "# dummy database\n",
    "\n",
    "db_icd10 = [\n",
    "    \"I10 - Essential (primary) hypertension (Darah Tinggi)\",\n",
    "    \"E11 - Type 2 diabetes mellitus (Kencing Manis)\",\n",
    "    \"A01 - Typhoid fever (Tipes)\"\n",
    "]\n",
    "\n",
    "db_medis = [\n",
    "    \"Pedoman Hipertensi: Wajib observasi tekanan darah dan cek darah lengkap.\",\n",
    "    \"Pedoman Diabetes: Cek HbA1c dan Gula Darah Puasa.\",\n",
    "    \"Pedoman Tipes: Bed rest total dan pemberian antibiotik.\"\n",
    "]\n",
    "\n",
    "db_polis_rules = [\n",
    "    \"Pasal Hipertensi: Ditanggung penuh setelah masa tunggu 12 bulan.\",\n",
    "    \"Pasal Umum: Rawat inap minimal 6 jam.\",\n",
    "    \"Pasal Pengecualian: Kosmetik dan Estetika tidak ditanggung.\"\n",
    "]\n",
    "\n",
    "# First is Retrieval\n",
    "def semantic_search_rag(query, database, threshold=0.4):\n",
    "    db_vectors = model.encode(database)\n",
    "    query_vector = model.encode([query])\n",
    "    scores = cosine_similarity(query_vector, db_vectors)[0]\n",
    "    results = pd.DataFrame({'Data': database, 'Score': scores})\n",
    "    best_result = results.sort_values(by='Score', ascending=False).iloc[0]\n",
    "    if best_result['Score'] >= threshold:\n",
    "        return {\n",
    "            \"found\": True,\n",
    "            \"match_text\": best_result['Data'],\n",
    "            \"score\": float(best_result['Score'])\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"found\": False,\n",
    "            \"match_text\": \"Tidak ditemukan referensi yang cocok.\",\n",
    "            \"score\": float(best_result['Score'])\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "def main_function_rag():\n",
    "    query_diagnosa = dummyPayload['hospital']['clinical_justification']\n",
    "    print(query_diagnosa)\n",
    "\n",
    "main_function_rag()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
